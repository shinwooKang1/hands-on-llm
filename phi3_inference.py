import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline

# Check available devices
print("=== M4 Chip Optimization ===")
print(f"MPS available: {torch.backends.mps.is_available()}")
print(f"MPS built: {torch.backends.mps.is_built()}")

# For M4, we can use both CPU and GPU strategically
if torch.backends.mps.is_available():
    device = "mps"  # Use GPU for model inference
    cpu_device = "cpu"  # Use CPU for preprocessing
    print("âœ… Using M4 unified memory architecture")
    print("âœ… GPU (MPS) for model inference")
    print("âœ… CPU for data preprocessing")
else:
    device = "cpu"
    cpu_device = "cpu"
    print("âš ï¸  Using CPU only")

print(f"Main device: {device}")
print(f"CPU device: {cpu_device}")

# Load model and tokenizer
print("Loading tokenizer...")
tokenizer = AutoTokenizer.from_pretrained("microsoft/Phi-3-mini-4k-instruct")

print("Loading model...")
model = AutoModelForCausalLM.from_pretrained(
    "microsoft/Phi-3-mini-4k-instruct",
    device_map=device,
    dtype=torch.float16,  # Use float16 for better performance on MPS
    trust_remote_code=False,
)

# Create a pipeline (without device argument for accelerate-loaded models)
print("Creating pipeline...")
generator = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    return_full_text=False,
    max_new_tokens=50,
    do_sample=False,
)

# Example of using both CPU and GPU strategically
prompt = "Write an email apologizing to Sarah for the tragic gardening mishap. Explain how it happened."

print(f"\nGenerating text for prompt: {prompt}")

# CPUì—ì„œ í† í°í™” (ë¹ ë¥¸ ì „ì²˜ë¦¬)
print("ğŸ”„ Tokenizing on CPU...")
inputs = tokenizer(prompt, return_tensors="pt")

# GPUë¡œ ì´ë™ (í†µí•© ë©”ëª¨ë¦¬ë¡œ ë¹ ë¥¸ ì „ì†¡)
print("ğŸš€ Moving to GPU...")
if device == "mps":
    inputs = {k: v.to(device) for k, v in inputs.items()}

# GPUì—ì„œ ì¶”ë¡ 
print("âš¡ Running inference on GPU...")
with torch.no_grad():
    outputs = model.generate(
        **inputs,
        max_new_tokens=50,
        do_sample=False,
        pad_token_id=tokenizer.eos_token_id
    )

# CPUë¡œ ë‹¤ì‹œ ì´ë™í•˜ì—¬ ë””ì½”ë”©
print("ğŸ”„ Decoding on CPU...")
if device == "mps":
    outputs = outputs.to(cpu_device)

generated_text = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)

print("\n=== Generated Text ===")
print(generated_text)

print("\n=== Performance Summary ===")
print("âœ… Utilized M4 unified memory architecture")
print("âœ… CPU-GPU collaboration for optimal performance")
